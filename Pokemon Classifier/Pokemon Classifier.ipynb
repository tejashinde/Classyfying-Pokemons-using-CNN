{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/tejasshinde/.local/lib/python3.6/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.25.3) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    " \n",
    "# import the necessary packages\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import SGD\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "import pickle\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagePaths = sorted(list(paths.list_images('images/')))\n",
    "random.seed(42)\n",
    "random.shuffle(imagePaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['images/bulbasaur/00000016.png',\n",
       " 'images/squirtle/00000041.jpg',\n",
       " 'images/pikachu/00000010.png',\n",
       " 'images/pikachu/00000095.jpg',\n",
       " 'images/pikachu/00000149.jpg']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagePaths[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize, Flatten and append to data,label lists\n",
    "### Flattening: 32x32x3 = 3072"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for imagePath in imagePaths:\n",
    "    image = cv2.imread(imagePath)\n",
    "    # flatten the image into 32x32x3=3072\n",
    "    image = cv2.resize(image, (32, 32)).flatten()\n",
    "    data.append(image)\n",
    " \n",
    "    label = imagePath.split(os.path.sep)[-2]\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale the raw pixel intensities to the range [0, 1] from [0, 255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Test split of 75, 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainX, testX, trainY, testY) = train_test_split(data,\n",
    "    labels, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "trainY = lb.fit_transform(trainY)\n",
    "testY = lb.transform(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1024, input_shape=(3072,), activation=\"sigmoid\"))\n",
    "model.add(Dense(512, activation=\"sigmoid\")) \n",
    "model.add(Dense(256, activation=\"sigmoid\")) \n",
    "# model.add(Dense(128, activation=\"sigmoid\")) \n",
    "model.add(Dense(5, activation=\"softmax\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training network...\n"
     ]
    }
   ],
   "source": [
    "INIT_LR = 0.1\n",
    "EPOCHS = 90\n",
    "\n",
    "print(\"[INFO] training network...\")\n",
    "opt = SGD(lr=INIT_LR)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "\tmetrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 720 samples, validate on 241 samples\n",
      "Epoch 1/90\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 1.8024 - acc: 0.2222 - val_loss: 1.6334 - val_acc: 0.1577\n",
      "Epoch 2/90\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 1.6185 - acc: 0.2319 - val_loss: 1.5960 - val_acc: 0.2407\n",
      "Epoch 3/90\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 1.5926 - acc: 0.2611 - val_loss: 1.5798 - val_acc: 0.2199\n",
      "Epoch 4/90\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 1.5059 - acc: 0.2917 - val_loss: 1.4161 - val_acc: 0.2988\n",
      "Epoch 5/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 1.2969 - acc: 0.3875 - val_loss: 1.0983 - val_acc: 0.4481\n",
      "Epoch 6/90\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 1.2007 - acc: 0.4153 - val_loss: 1.1324 - val_acc: 0.3402\n",
      "Epoch 7/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 1.1363 - acc: 0.4528 - val_loss: 1.2817 - val_acc: 0.3817\n",
      "Epoch 8/90\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 1.0447 - acc: 0.4889 - val_loss: 1.1270 - val_acc: 0.4315\n",
      "Epoch 9/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 1.0023 - acc: 0.5208 - val_loss: 1.0187 - val_acc: 0.5477\n",
      "Epoch 10/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.9258 - acc: 0.5875 - val_loss: 0.7221 - val_acc: 0.7303\n",
      "Epoch 11/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.8635 - acc: 0.6083 - val_loss: 0.7680 - val_acc: 0.6307\n",
      "Epoch 12/90\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.7410 - acc: 0.6639 - val_loss: 0.5881 - val_acc: 0.7925\n",
      "Epoch 13/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.7233 - acc: 0.7028 - val_loss: 0.5671 - val_acc: 0.8091\n",
      "Epoch 14/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.6542 - acc: 0.7417 - val_loss: 0.5923 - val_acc: 0.7303\n",
      "Epoch 15/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.6558 - acc: 0.7556 - val_loss: 0.5725 - val_acc: 0.7884\n",
      "Epoch 16/90\n",
      "720/720 [==============================] - 3s 3ms/step - loss: 0.5868 - acc: 0.7708 - val_loss: 0.9882 - val_acc: 0.6017\n",
      "Epoch 17/90\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.5806 - acc: 0.7917 - val_loss: 0.5065 - val_acc: 0.8008\n",
      "Epoch 18/90\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.5462 - acc: 0.8139 - val_loss: 0.7190 - val_acc: 0.6971\n",
      "Epoch 19/90\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.5122 - acc: 0.8292 - val_loss: 0.8042 - val_acc: 0.6763\n",
      "Epoch 20/90\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.4459 - acc: 0.8431 - val_loss: 0.8000 - val_acc: 0.6722\n",
      "Epoch 21/90\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.4670 - acc: 0.8347 - val_loss: 0.5141 - val_acc: 0.7842\n",
      "Epoch 22/90\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.4176 - acc: 0.8653 - val_loss: 0.4956 - val_acc: 0.8133\n",
      "Epoch 23/90\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.3614 - acc: 0.8778 - val_loss: 0.5186 - val_acc: 0.8174\n",
      "Epoch 24/90\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.4041 - acc: 0.8750 - val_loss: 0.4626 - val_acc: 0.8382\n",
      "Epoch 25/90\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.3101 - acc: 0.8986 - val_loss: 1.3727 - val_acc: 0.6141\n",
      "Epoch 26/90\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.3968 - acc: 0.8806 - val_loss: 0.4623 - val_acc: 0.8465\n",
      "Epoch 27/90\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.3056 - acc: 0.8972 - val_loss: 0.5634 - val_acc: 0.7842\n",
      "Epoch 28/90\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.3335 - acc: 0.8819 - val_loss: 0.4717 - val_acc: 0.8299\n",
      "Epoch 29/90\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.3143 - acc: 0.8944 - val_loss: 0.5481 - val_acc: 0.7925\n",
      "Epoch 30/90\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.2660 - acc: 0.9278 - val_loss: 0.3686 - val_acc: 0.8880\n",
      "Epoch 31/90\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.2729 - acc: 0.9014 - val_loss: 0.5078 - val_acc: 0.8174\n",
      "Epoch 32/90\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.2976 - acc: 0.9139 - val_loss: 0.8421 - val_acc: 0.7593\n",
      "Epoch 33/90\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.2576 - acc: 0.9083 - val_loss: 0.5247 - val_acc: 0.8340\n",
      "Epoch 34/90\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.2583 - acc: 0.9236 - val_loss: 0.5010 - val_acc: 0.8257\n",
      "Epoch 35/90\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.2760 - acc: 0.9139 - val_loss: 0.4895 - val_acc: 0.8133\n",
      "Epoch 36/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.2099 - acc: 0.9333 - val_loss: 0.4934 - val_acc: 0.8589\n",
      "Epoch 37/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.2010 - acc: 0.9403 - val_loss: 0.4266 - val_acc: 0.8797\n",
      "Epoch 38/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.2321 - acc: 0.9250 - val_loss: 0.4282 - val_acc: 0.8631\n",
      "Epoch 39/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.2166 - acc: 0.9319 - val_loss: 1.6311 - val_acc: 0.6183\n",
      "Epoch 40/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.2434 - acc: 0.9250 - val_loss: 0.5790 - val_acc: 0.8133\n",
      "Epoch 41/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.2041 - acc: 0.9306 - val_loss: 0.6155 - val_acc: 0.8216\n",
      "Epoch 42/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.1576 - acc: 0.9542 - val_loss: 1.0767 - val_acc: 0.7386\n",
      "Epoch 43/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.1585 - acc: 0.9514 - val_loss: 0.4452 - val_acc: 0.8797\n",
      "Epoch 44/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.2473 - acc: 0.9347 - val_loss: 0.6730 - val_acc: 0.8008\n",
      "Epoch 45/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.1655 - acc: 0.9431 - val_loss: 0.4585 - val_acc: 0.8589\n",
      "Epoch 46/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.1613 - acc: 0.9375 - val_loss: 0.4481 - val_acc: 0.8589\n",
      "Epoch 47/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.1310 - acc: 0.9528 - val_loss: 0.4100 - val_acc: 0.8755\n",
      "Epoch 48/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.1530 - acc: 0.9542 - val_loss: 0.4945 - val_acc: 0.8672\n",
      "Epoch 49/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.1255 - acc: 0.9667 - val_loss: 0.3978 - val_acc: 0.8838\n",
      "Epoch 50/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.1445 - acc: 0.9472 - val_loss: 0.4451 - val_acc: 0.8755\n",
      "Epoch 51/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.1939 - acc: 0.9431 - val_loss: 0.3734 - val_acc: 0.9004\n",
      "Epoch 52/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.1226 - acc: 0.9639 - val_loss: 0.5185 - val_acc: 0.8423\n",
      "Epoch 53/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.1420 - acc: 0.9639 - val_loss: 0.4480 - val_acc: 0.8548\n",
      "Epoch 54/90\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.1563 - acc: 0.9500 - val_loss: 0.3688 - val_acc: 0.9087\n",
      "Epoch 55/90\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.1385 - acc: 0.9569 - val_loss: 0.4625 - val_acc: 0.8465\n",
      "Epoch 56/90\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.1253 - acc: 0.9597 - val_loss: 1.1985 - val_acc: 0.6722\n",
      "Epoch 57/90\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.1438 - acc: 0.9528 - val_loss: 0.3766 - val_acc: 0.9004\n",
      "Epoch 58/90\n",
      "720/720 [==============================] - 3s 3ms/step - loss: 0.1353 - acc: 0.9583 - val_loss: 0.4230 - val_acc: 0.8755\n",
      "Epoch 59/90\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.1604 - acc: 0.9556 - val_loss: 0.7577 - val_acc: 0.7510\n",
      "Epoch 60/90\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.1031 - acc: 0.9667 - val_loss: 0.5093 - val_acc: 0.8589\n",
      "Epoch 61/90\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.0951 - acc: 0.9750 - val_loss: 0.3978 - val_acc: 0.9004\n",
      "Epoch 62/90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720/720 [==============================] - 2s 3ms/step - loss: 0.0924 - acc: 0.9667 - val_loss: 0.3936 - val_acc: 0.8880\n",
      "Epoch 63/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.1709 - acc: 0.9542 - val_loss: 0.6360 - val_acc: 0.8299\n",
      "Epoch 64/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.1232 - acc: 0.9611 - val_loss: 0.7434 - val_acc: 0.7967\n",
      "Epoch 65/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.1839 - acc: 0.9403 - val_loss: 0.4687 - val_acc: 0.8755\n",
      "Epoch 66/90\n",
      "720/720 [==============================] - 3s 3ms/step - loss: 0.1367 - acc: 0.9514 - val_loss: 0.4917 - val_acc: 0.8631\n",
      "Epoch 67/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.1634 - acc: 0.9486 - val_loss: 0.4072 - val_acc: 0.8963\n",
      "Epoch 68/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.1249 - acc: 0.9611 - val_loss: 0.4715 - val_acc: 0.8755\n",
      "Epoch 69/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.0839 - acc: 0.9736 - val_loss: 0.4438 - val_acc: 0.8797\n",
      "Epoch 70/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.0705 - acc: 0.9792 - val_loss: 0.4249 - val_acc: 0.8921\n",
      "Epoch 71/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.0727 - acc: 0.9736 - val_loss: 0.4271 - val_acc: 0.9087\n",
      "Epoch 72/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.0678 - acc: 0.9806 - val_loss: 0.5830 - val_acc: 0.8548\n",
      "Epoch 73/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.0771 - acc: 0.9736 - val_loss: 0.4158 - val_acc: 0.8963\n",
      "Epoch 74/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.1341 - acc: 0.9583 - val_loss: 0.4635 - val_acc: 0.8838\n",
      "Epoch 75/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.0759 - acc: 0.9736 - val_loss: 0.4271 - val_acc: 0.9004\n",
      "Epoch 76/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.0814 - acc: 0.9792 - val_loss: 1.0743 - val_acc: 0.7344\n",
      "Epoch 77/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.0814 - acc: 0.9764 - val_loss: 0.4871 - val_acc: 0.8838\n",
      "Epoch 78/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.0608 - acc: 0.9819 - val_loss: 0.4363 - val_acc: 0.8880\n",
      "Epoch 79/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.2495 - acc: 0.9347 - val_loss: 0.4512 - val_acc: 0.8838\n",
      "Epoch 80/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.0766 - acc: 0.9736 - val_loss: 0.4523 - val_acc: 0.8921\n",
      "Epoch 81/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.0868 - acc: 0.9708 - val_loss: 0.4326 - val_acc: 0.8880\n",
      "Epoch 82/90\n",
      "720/720 [==============================] - 3s 3ms/step - loss: 0.0708 - acc: 0.9736 - val_loss: 0.5002 - val_acc: 0.8838\n",
      "Epoch 83/90\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.0631 - acc: 0.9792 - val_loss: 0.4771 - val_acc: 0.8797\n",
      "Epoch 84/90\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.0986 - acc: 0.9708 - val_loss: 0.8553 - val_acc: 0.7801\n",
      "Epoch 85/90\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.0454 - acc: 0.9861 - val_loss: 0.5019 - val_acc: 0.8631\n",
      "Epoch 86/90\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.0644 - acc: 0.9750 - val_loss: 0.4119 - val_acc: 0.8963\n",
      "Epoch 87/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.0569 - acc: 0.9792 - val_loss: 0.4348 - val_acc: 0.9004\n",
      "Epoch 88/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.0646 - acc: 0.9764 - val_loss: 0.4768 - val_acc: 0.8838\n",
      "Epoch 89/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.0424 - acc: 0.9819 - val_loss: 0.4757 - val_acc: 0.8880\n",
      "Epoch 90/90\n",
      "720/720 [==============================] - 2s 3ms/step - loss: 0.0560 - acc: 0.9833 - val_loss: 0.4328 - val_acc: 0.9129\n"
     ]
    }
   ],
   "source": [
    "H = model.fit(trainX, trainY, validation_data=(testX, testY),\n",
    "\tepochs=EPOCHS, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H.history['val_acc'][-4]\n",
    "observation_list = [EPOCHS,INIT_LR, H.history['loss'][-4], H.history['val_loss'][-4], H.history['acc'][-4], H.history['val_acc'][-4]]\n",
    "observation_list\n",
    "with open('./observations/observation_model.txt', 'a') as file_out:\n",
    "    file_out.write(str(observation_list) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## CNN\n",
    "\n",
    "\n",
    "### Conv2D( number_of_filters, (size_of_filter) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = open('./observations/observation.csv' , 'w')\n",
    "headers = 'EPOCH,INITIAL LEARNING RATE,BATCH SIZE,LOSS,VAL_LOSS,ACCURACY,VAL_ACC\\n'\n",
    "out.write(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymNet:\n",
    "\t@staticmethod\n",
    "\tdef build(width, height, depth, classes):\n",
    "\t\t# initialize the model along with the input shape to be\n",
    "\t\t# \"channels last\" and the channels dimension itself\n",
    "\t\tmodel = Sequential()\n",
    "\t\tinputShape = (height, width, depth)\n",
    "\t\tchanDim = -1\n",
    "\n",
    "\t\t# if we are using \"channels first\", update the input shape\n",
    "\t\t# and channels dimension\n",
    "\t\tif K.image_data_format() == \"channels_first\":\n",
    "\t\t\tinputShape = (depth, height, width)\n",
    "\t\t\tchanDim = 1\n",
    "        \n",
    "        # CONV => RELU => POOL layer set\n",
    "\t\tmodel.add(Conv2D(32, (3, 3), padding=\"same\",\n",
    "\t\t\tinput_shape=inputShape))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "\t\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\t\tmodel.add(Dropout(0.25))\n",
    "        \n",
    "        # (CONV => RELU) * 2 => POOL layer set\n",
    "\t\tmodel.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "\t\tmodel.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "\t\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\t\tmodel.add(Dropout(0.25))    \n",
    "        \n",
    "        # first (and only) set of FC => RELU layers\n",
    "\t\tmodel.add(Flatten())\n",
    "\t\tmodel.add(Dense(512))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization())\n",
    "\t\tmodel.add(Dropout(0.5))\n",
    "        \n",
    "\t\t# softmax classifier\n",
    "\t\tmodel.add(Dense(5))\n",
    "\t\tmodel.add(Activation(\"softmax\"))\n",
    "\n",
    "\t\t# return the constructed network architecture\n",
    "\t\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SymNet:\n",
    "# \t@staticmethod\n",
    "# \tdef build(width, height, depth, classes):\n",
    "# \t\t# initialize the model along with the input shape to be\n",
    "# \t\t# \"channels last\" and the channels dimension itself\n",
    "# \t\tmodel = Sequential()\n",
    "# \t\tinputShape = (height, width, depth)\n",
    "# \t\tchanDim = -1\n",
    "\n",
    "# \t\t# if we are using \"channels first\", update the input shape\n",
    "# \t\t# and channels dimension\n",
    "# \t\tif K.image_data_format() == \"channels_first\":\n",
    "# \t\t\tinputShape = (depth, height, width)\n",
    "# \t\t\tchanDim = 1\n",
    "        \n",
    "#         # CONV => RELU => POOL layer set\n",
    "# \t\tmodel.add(Conv2D(64, (3, 3), padding=\"same\",\n",
    "# \t\t\tinput_shape=inputShape))\n",
    "# \t\tmodel.add(Activation(\"relu\"))\n",
    "# \t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "# \t\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# \t\tmodel.add(Dropout(0.25))\n",
    "        \n",
    "#         # (CONV => RELU) * 2 => POOL layer set\n",
    "# \t\tmodel.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "# \t\tmodel.add(Activation(\"relu\"))\n",
    "# \t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "# \t\tmodel.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "# \t\tmodel.add(Activation(\"relu\"))\n",
    "# \t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "# \t\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# \t\tmodel.add(Dropout(0.25))    \n",
    "        \n",
    "# \t\tmodel.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "# \t\tmodel.add(Activation(\"relu\"))\n",
    "# \t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "# \t\tmodel.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "# \t\tmodel.add(Activation(\"relu\"))\n",
    "# \t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "# \t\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# \t\tmodel.add(Dropout(0.25))    \n",
    "        \n",
    "        \n",
    "# \t\tmodel.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "# \t\tmodel.add(Activation(\"relu\"))\n",
    "# \t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "# \t\tmodel.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "# \t\tmodel.add(Activation(\"relu\"))\n",
    "# \t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "# \t\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# \t\tmodel.add(Dropout(0.25))    \n",
    "        \n",
    "#         # first (and only) set of FC => RELU layers\n",
    "# \t\tmodel.add(Flatten())\n",
    "# \t\tmodel.add(Dense(512))\n",
    "# \t\tmodel.add(Activation(\"relu\"))\n",
    "# \t\tmodel.add(BatchNormalization())\n",
    "# \t\tmodel.add(Dropout(0.5))\n",
    "        \n",
    "# \t\t# softmax classifier\n",
    "# \t\tmodel.add(Dense(5))\n",
    "# \t\tmodel.add(Activation(\"softmax\"))\n",
    "\n",
    "# \t\t# return the constructed network architecture\n",
    "# \t\treturn model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Normalization is used to normalize the activations of a given input volume before passing it to the next layer in the network. It has been proven to be very effective at reducing the number of epochs required to train a CNN as well as stabilizing training itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "symnet = SymNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "\n",
    "imagePaths = sorted(list(paths.list_images('images/')))\n",
    "random.seed(42)\n",
    "random.shuffle(imagePaths)\n",
    "\n",
    "\n",
    "for imagePath in imagePaths:\n",
    "\timage = cv2.imread(imagePath)\n",
    "\timage = cv2.resize(image, (64, 64))\n",
    "\tdata.append(image)\n",
    "\n",
    "\tlabel = imagePath.split(os.path.sep)[-2]\n",
    "\tlabels.append(label)\n",
    "\n",
    "\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data and Binarize Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainX, testX, trainY, testY) = train_test_split(data,\n",
    "\tlabels, test_size=0.25, random_state=42)\n",
    "# print(trainY)\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "trainY = lb.fit_transform(trainY)\n",
    "testY = lb.transform(testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the image generator for data augmentation\n",
    "aug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1,\n",
    "\theight_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n",
    "\thorizontal_flip=True, fill_mode=\"nearest\")\n",
    "\n",
    "# initialize our VGG-like Convolutional Neural Network (64,64,3)\n",
    "model = symnet.build(width=64, height=64, depth=3,\n",
    "\tclasses=len(lb.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training network...\n",
      "Epoch 1/75\n",
      "22/22 [==============================] - 15s 669ms/step - loss: 2.0877 - acc: 0.3381 - val_loss: 1.1268 - val_acc: 0.5187\n",
      "Epoch 2/75\n",
      "22/22 [==============================] - 13s 610ms/step - loss: 1.6348 - acc: 0.4574 - val_loss: 0.8380 - val_acc: 0.6639\n",
      "Epoch 3/75\n",
      "22/22 [==============================] - 14s 641ms/step - loss: 1.2759 - acc: 0.5797 - val_loss: 0.6002 - val_acc: 0.7676\n",
      "Epoch 4/75\n",
      "22/22 [==============================] - 13s 604ms/step - loss: 1.2930 - acc: 0.5483 - val_loss: 0.5478 - val_acc: 0.7801\n",
      "Epoch 5/75\n",
      "22/22 [==============================] - 14s 615ms/step - loss: 1.0404 - acc: 0.6477 - val_loss: 0.5239 - val_acc: 0.7925\n",
      "Epoch 6/75\n",
      "22/22 [==============================] - 14s 616ms/step - loss: 1.0128 - acc: 0.6548 - val_loss: 0.5494 - val_acc: 0.7801\n",
      "Epoch 7/75\n",
      "22/22 [==============================] - 13s 597ms/step - loss: 0.9066 - acc: 0.7131 - val_loss: 0.5217 - val_acc: 0.7925\n",
      "Epoch 8/75\n",
      "22/22 [==============================] - 14s 634ms/step - loss: 0.9307 - acc: 0.6720 - val_loss: 0.7338 - val_acc: 0.7303\n",
      "Epoch 9/75\n",
      "22/22 [==============================] - 14s 645ms/step - loss: 0.7907 - acc: 0.7428 - val_loss: 0.3238 - val_acc: 0.8963\n",
      "Epoch 10/75\n",
      "22/22 [==============================] - 13s 611ms/step - loss: 0.9279 - acc: 0.7204 - val_loss: 0.4124 - val_acc: 0.8589\n",
      "Epoch 11/75\n",
      "22/22 [==============================] - 13s 596ms/step - loss: 0.7915 - acc: 0.7289 - val_loss: 0.4296 - val_acc: 0.8423\n",
      "Epoch 12/75\n",
      "22/22 [==============================] - 13s 609ms/step - loss: 0.7339 - acc: 0.7656 - val_loss: 0.3539 - val_acc: 0.8838\n",
      "Epoch 13/75\n",
      "22/22 [==============================] - 13s 613ms/step - loss: 0.5714 - acc: 0.8009 - val_loss: 0.3701 - val_acc: 0.8963\n",
      "Epoch 14/75\n",
      "22/22 [==============================] - 13s 609ms/step - loss: 0.6579 - acc: 0.8053 - val_loss: 0.3490 - val_acc: 0.8880\n",
      "Epoch 15/75\n",
      "22/22 [==============================] - 14s 625ms/step - loss: 0.6539 - acc: 0.7812 - val_loss: 0.3474 - val_acc: 0.8880\n",
      "Epoch 16/75\n",
      "22/22 [==============================] - 14s 619ms/step - loss: 0.6457 - acc: 0.8025 - val_loss: 0.3569 - val_acc: 0.8963\n",
      "Epoch 17/75\n",
      "22/22 [==============================] - 14s 640ms/step - loss: 0.6916 - acc: 0.7786 - val_loss: 0.3586 - val_acc: 0.8755\n",
      "Epoch 18/75\n",
      "22/22 [==============================] - 13s 604ms/step - loss: 0.6193 - acc: 0.7814 - val_loss: 0.3389 - val_acc: 0.8963\n",
      "Epoch 19/75\n",
      "22/22 [==============================] - 14s 615ms/step - loss: 0.6764 - acc: 0.7925 - val_loss: 0.3753 - val_acc: 0.8548\n",
      "Epoch 20/75\n",
      "22/22 [==============================] - 13s 599ms/step - loss: 0.6249 - acc: 0.8040 - val_loss: 0.3182 - val_acc: 0.9087\n",
      "Epoch 21/75\n",
      "22/22 [==============================] - 14s 622ms/step - loss: 0.6554 - acc: 0.7967 - val_loss: 0.3053 - val_acc: 0.9087\n",
      "Epoch 22/75\n",
      "22/22 [==============================] - 14s 625ms/step - loss: 0.5574 - acc: 0.8111 - val_loss: 0.2899 - val_acc: 0.9087\n",
      "Epoch 23/75\n",
      "22/22 [==============================] - 14s 643ms/step - loss: 0.5837 - acc: 0.8225 - val_loss: 0.3633 - val_acc: 0.8797\n",
      "Epoch 24/75\n",
      "22/22 [==============================] - 14s 634ms/step - loss: 0.6016 - acc: 0.8168 - val_loss: 0.3223 - val_acc: 0.8921\n",
      "Epoch 25/75\n",
      "22/22 [==============================] - 14s 618ms/step - loss: 0.5701 - acc: 0.8239 - val_loss: 0.3613 - val_acc: 0.8797\n",
      "Epoch 26/75\n",
      "22/22 [==============================] - 14s 631ms/step - loss: 0.5765 - acc: 0.8238 - val_loss: 0.3897 - val_acc: 0.8921\n",
      "Epoch 27/75\n",
      "22/22 [==============================] - 13s 611ms/step - loss: 0.5371 - acc: 0.8296 - val_loss: 0.3713 - val_acc: 0.8838\n",
      "Epoch 28/75\n",
      "22/22 [==============================] - 14s 620ms/step - loss: 0.4874 - acc: 0.8365 - val_loss: 0.3192 - val_acc: 0.9004\n",
      "Epoch 29/75\n",
      "22/22 [==============================] - 14s 618ms/step - loss: 0.5445 - acc: 0.8380 - val_loss: 0.3003 - val_acc: 0.8963\n",
      "Epoch 30/75\n",
      "22/22 [==============================] - 14s 619ms/step - loss: 0.4834 - acc: 0.8438 - val_loss: 0.4017 - val_acc: 0.8340\n",
      "Epoch 31/75\n",
      "22/22 [==============================] - 13s 613ms/step - loss: 0.5669 - acc: 0.8198 - val_loss: 0.2823 - val_acc: 0.9004\n",
      "Epoch 32/75\n",
      "22/22 [==============================] - 14s 619ms/step - loss: 0.5255 - acc: 0.8382 - val_loss: 0.2994 - val_acc: 0.9046\n",
      "Epoch 33/75\n",
      "22/22 [==============================] - 13s 587ms/step - loss: 0.4904 - acc: 0.8353 - val_loss: 0.3448 - val_acc: 0.8631\n",
      "Epoch 34/75\n",
      "22/22 [==============================] - 14s 627ms/step - loss: 0.5175 - acc: 0.8367 - val_loss: 0.3425 - val_acc: 0.8631\n",
      "Epoch 35/75\n",
      "22/22 [==============================] - 14s 652ms/step - loss: 0.4931 - acc: 0.8209 - val_loss: 0.4116 - val_acc: 0.8506\n",
      "Epoch 36/75\n",
      "22/22 [==============================] - 14s 636ms/step - loss: 0.4201 - acc: 0.8536 - val_loss: 0.3602 - val_acc: 0.8880\n",
      "Epoch 37/75\n",
      "22/22 [==============================] - 14s 618ms/step - loss: 0.5501 - acc: 0.8268 - val_loss: 0.3212 - val_acc: 0.8838\n",
      "Epoch 38/75\n",
      "22/22 [==============================] - 15s 669ms/step - loss: 0.4695 - acc: 0.8409 - val_loss: 0.3439 - val_acc: 0.8838\n",
      "Epoch 39/75\n",
      "22/22 [==============================] - 16s 724ms/step - loss: 0.5042 - acc: 0.8395 - val_loss: 0.3229 - val_acc: 0.9004\n",
      "Epoch 40/75\n",
      "22/22 [==============================] - 16s 745ms/step - loss: 0.4212 - acc: 0.8510 - val_loss: 0.3025 - val_acc: 0.8963\n",
      "Epoch 41/75\n",
      "22/22 [==============================] - 15s 680ms/step - loss: 0.5267 - acc: 0.8294 - val_loss: 0.2917 - val_acc: 0.9046\n",
      "Epoch 42/75\n",
      "22/22 [==============================] - 17s 764ms/step - loss: 0.4778 - acc: 0.8525 - val_loss: 0.3673 - val_acc: 0.8797\n",
      "Epoch 43/75\n",
      "22/22 [==============================] - 15s 691ms/step - loss: 0.4496 - acc: 0.8466 - val_loss: 0.3042 - val_acc: 0.8838\n",
      "Epoch 44/75\n",
      "22/22 [==============================] - 15s 680ms/step - loss: 0.3745 - acc: 0.8892 - val_loss: 0.3914 - val_acc: 0.8714\n",
      "Epoch 45/75\n",
      "22/22 [==============================] - 14s 629ms/step - loss: 0.4897 - acc: 0.8396 - val_loss: 0.3654 - val_acc: 0.8838\n",
      "Epoch 46/75\n",
      "22/22 [==============================] - 15s 667ms/step - loss: 0.4248 - acc: 0.8579 - val_loss: 0.3413 - val_acc: 0.8714\n",
      "Epoch 47/75\n",
      "22/22 [==============================] - 15s 668ms/step - loss: 0.3760 - acc: 0.8608 - val_loss: 0.3616 - val_acc: 0.8838\n",
      "Epoch 48/75\n",
      "22/22 [==============================] - 14s 640ms/step - loss: 0.4202 - acc: 0.8651 - val_loss: 0.4058 - val_acc: 0.8548\n",
      "Epoch 49/75\n",
      "22/22 [==============================] - 14s 643ms/step - loss: 0.4758 - acc: 0.8539 - val_loss: 0.3012 - val_acc: 0.9087\n",
      "Epoch 50/75\n",
      "22/22 [==============================] - 14s 621ms/step - loss: 0.4008 - acc: 0.8692 - val_loss: 0.4094 - val_acc: 0.8548\n",
      "Epoch 51/75\n",
      "22/22 [==============================] - 14s 655ms/step - loss: 0.3920 - acc: 0.8780 - val_loss: 0.2746 - val_acc: 0.9046\n",
      "Epoch 52/75\n",
      "22/22 [==============================] - 14s 651ms/step - loss: 0.3826 - acc: 0.8978 - val_loss: 0.2967 - val_acc: 0.9004\n",
      "Epoch 53/75\n",
      "22/22 [==============================] - 14s 648ms/step - loss: 0.4306 - acc: 0.8665 - val_loss: 0.2722 - val_acc: 0.9170\n",
      "Epoch 54/75\n",
      "22/22 [==============================] - 14s 646ms/step - loss: 0.3843 - acc: 0.8779 - val_loss: 0.2882 - val_acc: 0.9129\n",
      "Epoch 55/75\n",
      "22/22 [==============================] - 14s 656ms/step - loss: 0.4132 - acc: 0.8765 - val_loss: 0.2856 - val_acc: 0.9129\n",
      "Epoch 56/75\n",
      "22/22 [==============================] - 13s 611ms/step - loss: 0.4139 - acc: 0.8720 - val_loss: 0.2836 - val_acc: 0.9087\n",
      "Epoch 57/75\n",
      "22/22 [==============================] - 14s 657ms/step - loss: 0.3671 - acc: 0.8879 - val_loss: 0.2966 - val_acc: 0.8921\n",
      "Epoch 58/75\n",
      "22/22 [==============================] - 14s 632ms/step - loss: 0.3140 - acc: 0.8990 - val_loss: 0.2931 - val_acc: 0.9087\n",
      "Epoch 59/75\n",
      "22/22 [==============================] - 15s 667ms/step - loss: 0.4196 - acc: 0.8480 - val_loss: 0.2808 - val_acc: 0.9170\n",
      "Epoch 60/75\n",
      "22/22 [==============================] - 14s 652ms/step - loss: 0.3586 - acc: 0.8808 - val_loss: 0.2813 - val_acc: 0.9046\n",
      "Epoch 61/75\n",
      "22/22 [==============================] - 14s 638ms/step - loss: 0.4423 - acc: 0.8650 - val_loss: 0.3053 - val_acc: 0.9046\n",
      "Epoch 62/75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 15s 674ms/step - loss: 0.3544 - acc: 0.8866 - val_loss: 0.4035 - val_acc: 0.8755\n",
      "Epoch 63/75\n",
      "22/22 [==============================] - 14s 654ms/step - loss: 0.4128 - acc: 0.8709 - val_loss: 0.2850 - val_acc: 0.9129\n",
      "Epoch 64/75\n",
      "22/22 [==============================] - 14s 650ms/step - loss: 0.3817 - acc: 0.8864 - val_loss: 0.2898 - val_acc: 0.9087\n",
      "Epoch 65/75\n",
      "22/22 [==============================] - 14s 643ms/step - loss: 0.3475 - acc: 0.8864 - val_loss: 0.3410 - val_acc: 0.9046\n",
      "Epoch 66/75\n",
      "22/22 [==============================] - 14s 620ms/step - loss: 0.3246 - acc: 0.9020 - val_loss: 0.2826 - val_acc: 0.9212\n",
      "Epoch 67/75\n",
      "22/22 [==============================] - 14s 641ms/step - loss: 0.4370 - acc: 0.8553 - val_loss: 0.2705 - val_acc: 0.9170\n",
      "Epoch 68/75\n",
      "22/22 [==============================] - 14s 650ms/step - loss: 0.4086 - acc: 0.8750 - val_loss: 0.4268 - val_acc: 0.8672\n",
      "Epoch 69/75\n",
      "22/22 [==============================] - 14s 635ms/step - loss: 0.4269 - acc: 0.8736 - val_loss: 0.2409 - val_acc: 0.9295\n",
      "Epoch 70/75\n",
      "22/22 [==============================] - 15s 663ms/step - loss: 0.3591 - acc: 0.8821 - val_loss: 0.2510 - val_acc: 0.9253\n",
      "Epoch 71/75\n",
      "22/22 [==============================] - 14s 630ms/step - loss: 0.3348 - acc: 0.8892 - val_loss: 0.2419 - val_acc: 0.9253\n",
      "Epoch 72/75\n",
      "22/22 [==============================] - 14s 657ms/step - loss: 0.3725 - acc: 0.8750 - val_loss: 0.2331 - val_acc: 0.9295\n",
      "Epoch 73/75\n",
      "22/22 [==============================] - 14s 617ms/step - loss: 0.3607 - acc: 0.8777 - val_loss: 0.2454 - val_acc: 0.9170\n",
      "Epoch 74/75\n",
      "22/22 [==============================] - 14s 631ms/step - loss: 0.3727 - acc: 0.8835 - val_loss: 0.2530 - val_acc: 0.9087\n",
      "Epoch 75/75\n",
      "22/22 [==============================] - 14s 616ms/step - loss: 0.3713 - acc: 0.8794 - val_loss: 0.2680 - val_acc: 0.9129\n"
     ]
    }
   ],
   "source": [
    "INIT_LR = 0.001\n",
    "EPOCHS = 75\n",
    "BS = 32\n",
    "\n",
    "print(\"[INFO] training network...\")\n",
    "opt = SGD(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "\tmetrics=[\"accuracy\"])\n",
    "\n",
    "H = model.fit_generator(aug.flow(trainX, trainY, batch_size=BS),\n",
    "\tvalidation_data=(testX, testY), steps_per_epoch=len(trainX) // BS,\n",
    "\tepochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = open('./observations/observation.csv' , 'a')\n",
    "observation_list = [EPOCHS,INIT_LR,BS, H.history['loss'][-1], H.history['val_loss'][-1], H.history['acc'][-1], H.history['val_acc'][-1]]\n",
    "observation_list\n",
    "with open('./observations/observation.csv', 'a') as file_out:\n",
    "    file_out.write(str(observation_list) + '\\n')\n",
    "# out.write(EPOCHS + ',' + INIT_LR + ',' + BS + ',' + H.history['loss'][-1] + ',' + H.history['val_loss'][-1] + ',' + H.history['acc'][-1] + ',' + H.history['val_acc'][-1] + '\\n')\n",
    "# out.write(str(observation_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating network...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   bulbasaur       0.96      0.96      0.96        53\n",
      "  charmander       0.89      0.92      0.90        51\n",
      "      mewtwo       0.94      0.96      0.95        52\n",
      "     pikachu       0.86      0.94      0.90        47\n",
      "    squirtle       0.90      0.74      0.81        38\n",
      "\n",
      "    accuracy                           0.91       241\n",
      "   macro avg       0.91      0.90      0.91       241\n",
      "weighted avg       0.91      0.91      0.91       241\n",
      "\n",
      "[INFO] serializing network and label binarizer...\n"
     ]
    }
   ],
   "source": [
    "# evaluate the network\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model.predict(testX, batch_size=32)\n",
    "print(classification_report(testY.argmax(axis=1),\n",
    "\tpredictions.argmax(axis=1), target_names=lb.classes_))\n",
    "\n",
    "N = np.arange(0, EPOCHS)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(N, H.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(N, H.history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy (SmallVGGNet)\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()\n",
    "plt.savefig('symnet_results.png')\n",
    "\n",
    "print(\"[INFO] serializing network and label binarizer...\")\n",
    "model.save('symnet.model')\n",
    "f = open('labels.bin', \"wb\")\n",
    "f.write(pickle.dumps(lb))\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
